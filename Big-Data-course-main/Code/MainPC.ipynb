{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MainPC.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import FloatType\n","from pyspark.ml import Pipeline\n","from pyspark.ml.feature import StringIndexer, VectorAssembler \n","from pyspark.ml.regression import LinearRegression, RandomForestRegressor, DecisionTreeRegressor, GBTRegressor \n","from pyspark.ml.evaluation import RegressionEvaluator\n","from sklearn.model_selection import train_test_split\n","\n","from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"],"metadata":{"id":"m8YHb_hYlkFj","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"81a9f5a2-cea7-4b14-b568-f65786b6417e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b7eca3bffa9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFloatType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["def prepro(data):\n","    \n","    data['StateHoliday'] = data.StateHoliday.astype('category')\n","    tempdf = pd.get_dummies(data['StateHoliday'])\n","    hol = ['0 holiday', 'a holiday', 'b holiday', 'c holiday']\n","    temp0 = ['0', 'a', 'b', 'c']\n","    for h in range(len(hol)):\n","        data[hol[h]] = tempdf[temp0[h]]\n","    data = data[data.Open == 1].copy()\n","\n","def datesplitandcomdura(data):\n","    \n","    data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n","    data[\"Year\"] = data[\"Date\"].dt.year\n","    data[\"Month\"] = data[\"Date\"].dt.month\n","    data[\"Day\"] = data[\"Date\"].dt.day\n","    data[\"WeekOfYear\"] = data[\"Date\"].dt.isocalendar().week\n","    data['CompetitionOpenTime'] = 12 * (data.Year - data.CompetitionOpenSinceYear) \\\n","                                  + (data.Month - data.CompetitionOpenSinceMonth)\n","    data['CompetitionOpenTime'] = data['CompetitionOpenTime'].apply(lambda x: 0 if x < 0 else x).fillna(0)\n","\n","def nalfil(data):\n","    \n","    data['Promo'] = data['Promo'].apply(lambda x: 0 if x < 0 else x).fillna(0)\n","    data['Promo2'] = data['Promo2'].apply(lambda x: 0 if x < 0 else x).fillna(0)\n","    data['PromoInterval'] = data['PromoInterval'].fillna(0)\n","\n","def check_promo_month(row):  # check if promo is given in the particular month\n","    \n","    month2str = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun',\n","                 7: 'Jul', 8: 'Aug', 9: 'Sept', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n","    try:\n","        months = (row['PromoInterval'] or '').split(',')\n","        if row['Promo2Open'] and month2str[row['Month']] in months:\n","            return 1\n","        else:\n","            return 0\n","    except:\n","        return 0\n","\n","def promo_cols(data):  # calculate duration of promotion(in months)\n","    \n","    data['Promo2Open'] = 12 * (data.Year - data.Promo2SinceYear) + (data.WeekOfYear - data.Promo2SinceWeek) * 7 / 30\n","    data['Promo2Open'] = data['Promo2Open'].apply(lambda x: 0 if x < 0 else x).fillna(0) * data[\n","        'Promo2']  # only when there is promo\n","    # whether a new round of promotion started in curent month\n","    data['IsPromo2Month'] = data.apply(check_promo_month, axis=1) * data['Promo2']\n","\n","def compdis(data):\n","    \n","    max_comp_dist = data['CompetitionDistance'].max()\n","    data['CompetitionDistance'] = data['CompetitionDistance'].fillna(max_comp_dist)\n","\n","def assandstoty(data):\n","    \n","    data['Assortment'] = data.Assortment.astype('category')\n","    data['StoreType'] = data.StoreType.astype('category')\n","    tempadf = pd.get_dummies(data['Assortment'])\n","    tempsdf = pd.get_dummies(data['StoreType'])\n","\n","    ass = ['a ass', 'b ass', 'c ass']\n","    tempas = ['a', 'b', 'c']\n","    for a in range(len(ass)):\n","        data[ass[a]] = tempadf[tempas[a]]\n","\n","    stoty = ['type a', 'type b', 'type c', 'type d']\n","    tempst = ['a', 'b', 'c', 'd']\n","    for s in range(len(stoty)):\n","        data[stoty[s]] = tempsdf[tempst[s]]\n","\n","def popo(data):\n","    \n","    data.pop('Assortment')  # Preprocessed\n","    data.pop('StateHoliday')  # Preprocessed\n","    data.pop('StoreType')  # Preprocessed\n","    data.pop('PromoInterval')  # Preprocessed\n","    data.pop('Date')  # Preprocessed\n","    data.pop('CompetitionOpenSinceMonth')  # More than 50% NaN\n","    data.pop('CompetitionOpenSinceYear')  # More than 50% NaN\n","    data.pop('Promo2SinceYear')  # More than 50% NaN\n","    data.pop('Promo2SinceWeek')  # More than 50% NaN\n","    data.pop('Promo2')\n","    data.pop('type b')\n","    data.pop('Customers')\n","\n","def prego(data):\n","    \n","    prepro(data)\n","    datesplitandcomdura(data)\n","    nalfil(data)\n","    promo_cols(data)\n","    compdis(data)\n","    assandstoty(data)\n","    popo(data)\n","       \n","def modbuil(data,  train, val):\n","    \n","    \n","    feature_list = []\n","    for col in data.columns:\n","        if col == 'Sales':\n","            continue\n","        else:\n","            feature_list.append(col)\n","\n","    vectorassembler = VectorAssembler(inputCols=feature_list, outputCol='features').setHandleInvalid(\"keep\")\n","    train_vector = vectorassembler.transform(data)\n","\n","    splits = train_vector.randomSplit([0.8, 0.2])\n","    train = splits[0]\n","    val = splits[1]\n","    return train, val\n","\n","def linreg(train, val, lr):\n","    \n","    paramGrid = ParamGridBuilder()\\\n","    .addGrid(lr.regParam, [0.5,0.1,0.05,0.01])\\\n","    .addGrid(lr.elasticNetParam, [0.3,0.1,0.05])\\\n","    .build()\n","    evaluatorr = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Sales\", metricName=\"r2\")\n","    tvs = TrainValidationSplit(estimator=lr,\n","                           estimatorParamMaps=paramGrid,\n","                           evaluator=evaluatorr,\n","                           # 80% of the data will be used for training, 20% for validation.\n","                           trainRatio=0.8)\n","    \n","    lr_model = tvs.fit(train)\n","    lr_evaluator = evaluatorr.evaluate(lr_model.transform(train))\n","    return lr_evaluator\n","\n","def renforeg(train, val, rf):\n","    \n","    paramGrid =ParamGridBuilder()\\\n","    .addGrid(rf.maxDepth, [10,20,30])\\\n","    .addGrid(rf.minInstancesPerNode, [1,2,3])\\\n","    .addGrid(rf.bootstrap, [True,False])\\\n","    .build()\n","    \n","    evaluatorr = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Sales\", metricName=\"r2\")\n","    tvs = TrainValidationSplit(estimator=rf,\n","                           estimatorParamMaps=paramGrid,\n","                           evaluator=evaluatorr,\n","                           # 80% of the data will be used for training, 20% for validation.\n","                           trainRatio=0.8)\n","    \n","    rf_model = tvs.fit(train)\n","    rf_evaluator = evaluatorr.evaluate(rf_model.transform(train))\n","    \n","    return rf_evaluator\n","\n","def dtreg(train, val, dt):\n","\n","    paramGrid =ParamGridBuilder()\\\n","    .addGrid(dt.maxDepth, [10,20,30])\\\n","    .addGrid(dt.minInstancesPerNode, [1,2,3])\\\n","    .build()\n","    \n","    evaluatorr = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Sales\", metricName=\"r2\")\n","    tvs = TrainValidationSplit(estimator=dt,\n","                           estimatorParamMaps=paramGrid,\n","                           evaluator=evaluatorr,\n","                           # 80% of the data will be used for training, 20% for validation.\n","                           trainRatio=0.8)\n","    \n","    dt_model = tvs.fit(train)\n","    dt_evaluator = evaluatorr.evaluate(dt_model.transform(train))\n","        \n","    return dt_evaluator\n","\n","def gbtreg(train, val, gbt):\n","\n","    paramGrid =ParamGridBuilder()\\\n","    .addGrid(gbt.maxDepth, [10,20,30])\\\n","    .addGrid(gbt.minInstancesPerNode, [1,2,3])\\\n","    .build()\n","    evaluatorr = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Sales\", metricName=\"r2\")\n","    tvs = TrainValidationSplit(estimator=gbt,\n","                           estimatorParamMaps=paramGrid,\n","                           evaluator=evaluatorr,\n","                           # 80% of the data will be used for training, 20% for validation.\n","                           trainRatio=0.8)\n","    \n","    gbt_model = tvs.fit(train)\n","    gbt_evaluator = evaluatorr.evaluate(gbt_model.transform(train))\n","        \n","    return [gbt_evaluator, gbt_model]\n","\n","\n","def mogo(train_in):\n","  \n","    lr = LinearRegression(featuresCol='features', labelCol='Sales', maxIter=10,\n","                          regParam=0.8, elasticNetParam=0.1)\n","    rf = RandomForestRegressor(featuresCol='features', labelCol='Sales', maxDepth=20,\n","                                    minInstancesPerNode=2, bootstrap=True)\n","    dt = DecisionTreeRegressor(featuresCol='features', labelCol='Sales', maxDepth=20,\n","                                    minInstancesPerNode=2)\n","    gbt = GBTRegressor(featuresCol='features', labelCol='Sales', maxDepth=20,\n","                                    minInstancesPerNode=2)\n","    train = []\n","    val = []\n","    \n","    t, v = modbuil(train_in, train, val)\n","    l = linreg(t, v, lr)\n","    r = renforeg(t, v,  rf)\n","    d = dtreg(t, v, dt)\n","    g = gbtreg(t, v, gbt)\n","    return l, r, d, g\n","    "],"metadata":{"id":"sxKuuakilzAR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def spgo(xtrain):\n","     \n","    xtrain['CompetitionDistance'] = xtrain['CompetitionDistance'].astype(int) \n","    xtrain['CompetitionOpenTime'] = xtrain['CompetitionOpenTime'].astype(int)\n","    xtrain['Promo2Open'] = xtrain['Promo2Open'].astype(int)\n","    #print(xtrain.iloc[-2:])\n","    \n","    \n","    mySchema = StructType([ StructField(\"Store\", IntegerType(), True)\\\n","                           ,StructField(\"DayOfWeek\", IntegerType(), True)\\\n","                           ,StructField(\"Sales\", IntegerType(), True)\\\n","                           ,StructField(\"Open\", IntegerType(), True)\\\n","                          ,StructField(\"Promo\" , IntegerType(), True)\\\n","                          ,StructField(\"SchoolHoliday\", IntegerType(), True)\\\n","                          ,StructField(\"CompetitionDistance\", IntegerType(), True)\\\n","                          ,StructField(\"0 holiday\", IntegerType(), True)\\\n","                          ,StructField(\"a holiday\", IntegerType(), True)\\\n","                          ,StructField(\"b holiday\", IntegerType(), True)\\\n","                          ,StructField(\"c holiday\", IntegerType(), True)\\\n","                          ,StructField(\"Year\", IntegerType(), True)\\\n","                          ,StructField(\"Month\", IntegerType(), True)\\\n","                          ,StructField(\"Day\", IntegerType(), True)\\\n","                          ,StructField(\"WeekOfYear\", IntegerType(), True)\\\n","                          ,StructField(\"CompetitionOpenTime\", IntegerType(), True)\\\n","                          ,StructField(\"Promo2Open\", IntegerType(), True)\\\n","                          ,StructField(\"IsPromo2Month\", IntegerType(), True)\\\n","                          ,StructField(\"a ass\", IntegerType(), True)\\\n","                          ,StructField(\"b ass\", IntegerType(), True)\\\n","                          ,StructField(\"c ass\", IntegerType(), True)\\\n","                          ,StructField(\"type a\", IntegerType(), True)\\\n","                          ,StructField(\"type c\", IntegerType(), True)\\\n","                          ,StructField(\"type d\", IntegerType(), True)])\n","                        \n","\n","    \n","    spark = SparkSession.builder.master(\"spark://master:7077\").getOrCreate()\n","    trdf = spark.createDataFrame(xtrain,schema=mySchema )\n","\n","    print(\"--------------made a spark df from the data------------\")\n","    \n","    return trdf"],"metadata":{"id":"-w-KLBRpYp3W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = pd.read_csv('x_train.csv')\n","y = x['Sales']\n","\n","\n","prego(x)\n","\n","x = x.iloc[:100]\n","\n","spdf = spgo(x)\n","l, r, d, g= mogo(spdf)\n","\n","\n","# print(l)\n","# print(' ')\n","# print(r)\n","# print(' ')\n","# print(d)\n","# print(' ')\n","# print(g[0])"],"metadata":{"id":"tvCXy78PmDTJ","colab":{"base_uri":"https://localhost:8080/","height":380},"outputId":"58a1fc8e-db05-49b1-baa9-ef85d4e7f27f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-be1a5e899d10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sales'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprego\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'x_train.csv'"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","\n","def addlabels(x, y):\n","            for i in range(len(x)):\n","                plt.text(i, round(y[i],5), round(y[i],5))\n","\n","\n","\n","\n","x_bar = ['rl', 'rf', 'dt', 'gbt']\n","y_bar = [l,r,d,g[0]]                \n","\n","addlabels(x_bar, y_bar)\n","\n","plt.bar(x_bar,y_bar)"],"metadata":{"id":"7KGwrbVmmE4h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import pyspark.sql.functions as F\n","from confluent_kafka import Producer, Consumer\n","from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType\n","import matplotlib.pyplot as plt\n","import time\n","\n","gbt_model= GBTRegressor(featuresCol='features', labelCol='Sales', maxDepth=20,\n","                                    minInstancesPerNode=2)\n","\n","P1 = Producer({'bootstrap.servers':' localhost:9092'})\n","\n","def forPred(new_data):\n","    \n","    \n","    new_data.collect()\n","\n","\n","def gbtreg(train,gbt_model,new_data):\n","   \n","    mySchema1 = StructType([ StructField(\"Store\", IntegerType(), True)\\\n","                           ,StructField(\"DayOfWeek\", IntegerType(), True)\\\n","                           ,StructField(\"Open\", IntegerType(), True)\\\n","                          ,StructField(\"Promo\" , IntegerType(), True)\\\n","                          ,StructField(\"SchoolHoliday\", IntegerType(), True)\\\n","                          ,StructField(\"CompetitionDistance\", IntegerType(), True)\\\n","                          ,StructField(\"0 holiday\", IntegerType(), True)\\\n","                          ,StructField(\"a holiday\", IntegerType(), True)\\\n","                          ,StructField(\"b holiday\", IntegerType(), True)\\\n","                          ,StructField(\"c holiday\", IntegerType(), True)\\\n","                          ,StructField(\"Year\", IntegerType(), True)\\\n","                          ,StructField(\"Month\", IntegerType(), True)\\\n","                          ,StructField(\"Day\", IntegerType(), True)\\\n","                          ,StructField(\"WeekOfYear\", IntegerType(), True)\\\n","                          ,StructField(\"CompetitionOpenTime\", IntegerType(), True)\\\n","                          ,StructField(\"Promo2Open\", IntegerType(), True)\\\n","                          ,StructField(\"IsPromo2Month\", IntegerType(), True)\\\n","                          ,StructField(\"a ass\", IntegerType(), True)\\\n","                          ,StructField(\"b ass\", IntegerType(), True)\\\n","                          ,StructField(\"c ass\", IntegerType(), True)\\\n","                          ,StructField(\"type a\", IntegerType(), True)\\\n","                          ,StructField(\"type c\", IntegerType(), True)\\\n","                          ,StructField(\"type d\", IntegerType(), True)])\n","    \n","    print(\"------- vector assembling START-----------\")\n","    print(\" \")\n","\n","    from pyspark.ml.feature import VectorAssembler\n","    \n","    vectorAssembler = VectorAssembler(inputCols = ['Store', 'DayOfWeek'\\\n","                                                   ,'Open', 'Promo', 'SchoolHoliday' \\\n","                                                   ,'CompetitionDistance', '0 holiday'\\\n","                                                   , 'a holiday', 'b holiday', 'c holiday'\\\n","                                                   , 'Year','Month','Day', 'WeekOfYear', 'CompetitionOpenTime'\\\n","                                                   , 'Promo2Open', 'IsPromo2Month', 'a ass', 'b ass', 'c ass'\\\n","                                                   , 'type a' , 'type c', 'type d'], outputCol = 'features')\n","    train = vectorAssembler.transform(train)\n","    new_data = vectorAssembler.transform(new_data)\n","    \n","    print(\"------- vector assembling OK-----------\")\n","    print(\" \")\n","    \n","\n","    \n","    paramGrid =ParamGridBuilder()\\\n","    .addGrid(gbt_model.maxDepth, [10,20,30])\\\n","    .addGrid(gbt_model.minInstancesPerNode, [1,2,3])\\\n","    .build()\n","    evaluatorr = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Sales\", metricName=\"r2\")\n","    tvs = TrainValidationSplit(estimator=gbt_model,\n","                           estimatorParamMaps=paramGrid,\n","                           evaluator=evaluatorr,\n","                           # 80% of the data will be used for training, 20% for validation.\n","                           trainRatio=0.8)\n","    \n","    print(\"------------------starting to train a new model-------------------------\")\n","    gbt_new = tvs.fit(train)\n","    gbt_evaluator = evaluatorr.evaluate(gbt_new.transform(train))\n","\n","    count = 0 \n","    \n","    mse_arr = []\n","    sale_arr = []\n","    \n","    for i in range(3):\n","        \n","        Sale = new_data.collect()[i]['Sales']\n","        pred = [new_data.select(['Store', 'DayOfWeek'\\\n","                                        ,'Open', 'Promo', 'SchoolHoliday' \\\n","                                        ,'CompetitionDistance', '0 holiday'\\\n","                                        , 'a holiday', 'b holiday', 'c holiday'\\\n","                                        , 'Year','Month','Day', 'WeekOfYear', 'CompetitionOpenTime'\\\n","                                        , 'Promo2Open', 'IsPromo2Month', 'a ass', 'b ass', 'c ass'\\\n","                                        , 'type a' , 'type c', 'type d']).collect()[i]]\n","        \n","        \n","        \n","        \n","        row1 = spark.createDataFrame(pred ,schema=mySchema1)\n","        \n","        time.sleep(10)\n","        \n","\n","        P1.produce(topic = str(int(row1.collect()[0]['Store'])) + \"a\",key='', \n","                   value = (\"prediction for next day \" + \" \" + str(gbt_new.transform(vectorAssembler.transform(row1)).collect()[0]['prediction'])).encode())\n","                   \n","        \n","        \n","        \n","        print(\"topic is \", str(int(row1.collect()[0]['Store'])) + \"a\")\n","        print(\"values is -------------- prediction for next day \" + \" \" + str(gbt_new.transform(vectorAssembler.transform(row1)).collect()[0]['prediction']))\n","\n","        print(\"-----sent-------------------\" + str(int(row1.collect()[0]['Store'])))\n","        \n","        \n","        model_pred = gbt_new.transform(vectorAssembler.transform(row1)).collect()[0]['prediction']\n","        \n","        print(Sale , \" - \", model_pred)\n","        mse_arr.append( Sale - model_pred)\n","        sale_arr.append(Sale)\n","        print(\"msearr -----------\")\n","        print(mse_arr)\n","        \n","    #calc the mse for the new samples\n","    # c = 0\n","    # k = 0\n","    # y_mean = sum(sale_arr)/len(sale_arr)\n","    \n","    # print(\"y_mean -----------\")\n","    # print(y_mean)\n","    \n","    # for j in sale_arr:\n","    #     k += (j - x_mean) ** 2 #Rtot\n","    \n","    \n","    # for i in mse_arr:\n","    #     print(i)\n","    #     print(\"printed i -------------\")\n","    #     c += i ** 2 #RSS\n","           \n","    # if(k == 0.0):\n","    #   RSquare = 0.0 #SHABAT\n","    # else:\n","    #   RSquare = c/k\n","    \n","    #c = c/len(mse_arr)   \n","    #print(c)\n","      \n","    time.sleep(10)\n","    \n","    meanErr = sum(mse_arr)/len(mse_arr)\n","\n","    return meanErr\n","\n","\n","\n","\n","def calculateNewModel(train_data,consumer,topic):\n","        \n","    running = True\n","    \n","    def appendRow(row1,train_data):\n","        data_to_append = {}\n","        for i in range(len(train_data.columns)):\n","            data_to_append[train_data.columns[i]] = int(float(row1[i]))\n","        train_data = train_data.append(data_to_append, ignore_index = True)\n","        return train_data\n","\n","\n","    def basic_consume_loop(consumer, topic, train_data):\n","        print(\"Starting basic consume loop\")\n","        res = []\n","        counter = 0\n","        \n","        new_data = pd.DataFrame(columns = x.columns)\n","        \n","        try:\n","            consumer.subscribe(topic)\n","\n","            while running:\n","                msg = consumer.poll(timeout=10.0)\n","                if msg is None: continue\n","\n","                if msg.error():\n","                    if msg.error().code() == KafkaError._PARTITION_EOF:\n","                        # End of partition event\n","                        sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n","                                         (msg.topic(), msg.partition(), msg.offset()))\n","                    elif msg.error():\n","                        raise KafkaException(msg.error())\n","                else:                    \n","                    \n","                    #print(msg.value())\n","                    m = msg.value().decode('utf-8').split(\",\")\n","                    if(m[0] == \"Sample\"):\n","                        print(\"got new msg\")\n","                        #print(m[1:])\n","                        #print(m[3:])\n","                        new_data = appendRow(m[3:], new_data)\n","                        train_data = appendRow(m[3:], train_data)\n","                        #print(type(train_data[\"Store\"].iloc[1]))\n","                        #print(train_data.iloc[-2:])\n","                        print('====ADDED====')\n","                        print(msg.value().decode('utf-8').split(','))\n","                        print('=============') \n","                        if(len(new_data)%3 == 0):\n","                            #print(len(new_data))\n","                            #print(new_data.iloc[::-1])\n","                            res.append(gbtreg(spgo(train_data),gbt_model,spgo(new_data)))\n","                            new_data = new_data[0:0]\n","                            \n","                            def addlabels(x, y):\n","                                for i in range(len(x)):\n","                                    plt.text(i, round(y[i],5), round(y[i],5))\n","                            x_bar = range(len(res))\n","                            y_bar = res               \n","                            addlabels(x_bar, y_bar)\n","                            plt.bar(x_bar,y_bar)\n","                            plt.show()\n","                            \n","                            \n","#                             print(\"res is ---------------\")\n","#                             print(res)\n","#                             for i in res:\n","#                                 print(\"difference = \" , i)\n","                               \n","        finally:\n","            # Close down consumer to commit final offsets.\n","            consumer.close()\n","\n","    def shutdown():\n","        running = False\n","\n","    basic_consume_loop(consumer, topic, train_data)\n","\n"],"metadata":{"id":"EyyLZEpxoLwy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from confluent_kafka import Consumer\n","\n","conf = {'bootstrap.servers': \"localhost:9092\",\n","        'group.id': \"foo\",\n","        'auto.offset.reset': 'smallest'}\n","\n","consumer1 = Consumer(conf)"],"metadata":{"id":"pIioRY6RoReJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LMZd0O8lW0T"},"outputs":[],"source":["calculateNewModel(x,consumer1,['1','2','3'])"]}]}